{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5ff5b70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ANÁLISIS COMPLETO - VALIDACIÓN Y LIMPIEZA DE DATOS\n",
      "======================================================================\n",
      "\n",
      "1. DIAGNÓSTICO INICIAL DEL DATASET\n",
      "----------------------------------------\n",
      "Dataset original: 450 filas, 7 columnas\n",
      "\n",
      "2. ANÁLISIS DE ESTRUCTURA DE DATOS\n",
      "----------------------------------------\n",
      " Verificando estructura de columnas...\n",
      " Columna 'student_info' podría contener datos en lugar de ser variable\n",
      " Columna 'G3' podría contener datos en lugar de ser variable\n",
      "\n",
      " Buscando columnas con múltiples variables...\n",
      "    Columna 'student_info' contiene múltiples variables separadas por ','\n",
      "    Columna 'studytime_absences' contiene múltiples variables separadas por '|'\n",
      "    Columna 'fecha_evaluacion' contiene múltiples variables separadas por '/'\n",
      "\n",
      "3. LIMPIEZA Y TRANSFORMACIÓN DE DATOS\n",
      "----------------------------------------\n",
      "Separando variables combinadas...\n",
      " Corrigiendo tipos de datos...\n",
      "Nuevo dataset: 450 filas, 10 columnas\n",
      "\n",
      "4. ANÁLISIS DE DATOS FALTANTES\n",
      "----------------------------------------\n",
      "Valores faltantes por columna:\n",
      "   G1: 56 (12.4%)\n",
      "   G2: 68 (15.1%)\n",
      "\n",
      "Aplicando estrategias de imputación...\n",
      "G1: imputado con media (12.14)\n",
      "G2: imputado con media (11.96)\n",
      "Valores faltantes después de limpieza: 0\n",
      "\n",
      "5. GESTIÓN DE DATOS DUPLICADO\n",
      "----------------------------------------\n",
      "Duplicados exactos encontrados: 50\n",
      "Duplicados por estudiante-curso: 50\n",
      "Filas eliminadas por duplicación: 50\n",
      "Dataset final: 400 filas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Maeva\\AppData\\Local\\Temp\\ipykernel_17760\\2962163341.py:138: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(impute_value, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# ANÁLISIS COMPLETO - LIMPIEZA Y VALIDACIÓN DE DATOS ESTUDIANTILES\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ANÁLISIS COMPLETO - VALIDACIÓN Y LIMPIEZA DE DATOS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# 1. CARGA Y DIAGNÓSTICO INICIAL\n",
    "print(\"\\n1. DIAGNÓSTICO INICIAL DEL DATASET\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Simulamos un dataset con múltiples problemas\n",
    "np.random.seed(42)\n",
    "n = 400\n",
    "\n",
    "data_problematico = {\n",
    "    # Combina nombre, curso y edad en una sola columna separados por comas - viola principio una variable por columna\n",
    "    'student_info': [f\"Estudiante_{i},Curso_{np.random.choice(['A','B','C'])},Edad_{np.random.randint(15,19)}\" for i in range(n)],\n",
    "    \n",
    "    # Combina horas de estudio y ausencias en texto separado por | - mezcla variables numéricas en string\n",
    "    'studytime_absences': [f\"{np.random.choice([1,2,3,4])}|{np.random.poisson(4)}\" for _ in range(n)],\n",
    "    \n",
    "    # Primera evaluación con 10% de valores como 'N/A' y formato decimal - missing values incorrectos\n",
    "    'G1': [f\"{np.clip(np.random.normal(12,3), 0, 20):.1f}\" if np.random.random() > 0.1 else 'N/A' for _ in range(n)],\n",
    "    \n",
    "    # Segunda evaluación con 15% de valores vacíos '' - inconsistencia en representación de missing values\n",
    "    'G2': [f\"{np.clip(np.random.normal(12,3), 0, 20):.1f}\" if np.random.random() > 0.15 else '' for _ in range(n)],\n",
    "    \n",
    "    # Tercera evaluación como float sin formatear - inconsistencia de tipos con G1 y G2\n",
    "    'G3': [np.clip(np.random.normal(12,3), 0, 20) for _ in range(n)],\n",
    "    \n",
    "    # Fechas como string en formato día/mes/año - dificulta ordenamiento y análisis temporal\n",
    "    'fecha_evaluacion': [(datetime(2023,1,1) + timedelta(days=np.random.randint(0,365))).strftime('%d/%m/%Y') for _ in range(n)],\n",
    "    \n",
    "    # Categorización manual de notas - pérdida de información y límites arbitrarios\n",
    "    'categoria_nota': ['Excelente' if x > 16 else 'Bueno' if x > 12 else 'Regular' if x > 8 else 'Insuficiente' for x in np.clip(np.random.normal(12,3,n), 0, 20)]\n",
    "}\n",
    "\n",
    "\n",
    "# Duplicamos algunas filas intencionalmente\n",
    "filas_duplicadas = [data_problematico[col][:50] for col in data_problematico.keys()]\n",
    "for i, col in enumerate(data_problematico.keys()):\n",
    "    data_problematico[col].extend(filas_duplicadas[i])\n",
    "\n",
    "df = pd.DataFrame(data_problematico)\n",
    "print(f\"Dataset original: {df.shape[0]} filas, {df.shape[1]} columnas\")\n",
    "\n",
    "# 2. ANÁLISIS DE ESTRUCTURA DE COLUMNAS\n",
    "print(\"\\n2. ANÁLISIS DE ESTRUCTURA DE DATOS\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# 2.1 Verificar si los nombres de columnas son valores en lugar de variables\n",
    "print(\" Verificando estructura de columnas...\")\n",
    "columnas_sospechosas = []\n",
    "for col in df.columns:\n",
    "   \n",
    "    if df[col].nunique() > len(df) * 0.8:\n",
    "        columnas_sospechosas.append(col)\n",
    "        print(f\" Columna '{col}' podría contener datos en lugar de ser variable\")\n",
    "# 2.2 Detectar columnas con múltiples variables\n",
    "print(\"\\n Buscando columnas con múltiples variables...\")\n",
    "for col in df.columns:\n",
    "    \n",
    "    ejemplos = df[col].dropna().head(3).tolist()\n",
    "    separadores = [',', '|', ';', '-', '/']\n",
    "    \n",
    "    for sep in separadores:\n",
    "        if any(sep in str(ejemplo) for ejemplo in ejemplos if pd.notna(ejemplo)):\n",
    "            print(f\"    Columna '{col}' contiene múltiples variables separadas por '{sep}'\")\n",
    "            break  # Rompe el loop interno cuando encuentra el primer separador\n",
    "\n",
    "# 3. LIMPIEZA DE DATOS COMPLETA\n",
    "print(\"\\n3. LIMPIEZA Y TRANSFORMACIÓN DE DATOS\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# 3.1 Separar columnas con múltiples variables\n",
    "print(\"Separando variables combinadas...\")\n",
    "\n",
    "df[['nombre_estudiante', 'curso', 'edad']] = df['student_info'].str.split(',', expand=True)\n",
    "df['edad'] = df['edad'].str.replace('Edad_', '').astype(float)\n",
    "df[['studytime', 'absences']] = df['studytime_absences'].str.split('|', expand=True)\n",
    "df['studytime'] = pd.to_numeric(df['studytime'], errors='coerce')\n",
    "df['absences'] = pd.to_numeric(df['absences'], errors='coerce')\n",
    "\n",
    "# 3.2 Limpiar y convertir tipos de datos\n",
    "print(\" Corrigiendo tipos de datos...\")\n",
    "\n",
    "df['G1'] = df['G1'].replace(['N/A', '', 'NULL', 'null'], np.nan)  # Unifica missing values en G1\n",
    "df['G2'] = df['G2'].replace(['', 'NULL', 'null'], np.nan)         # Estandariza valores nulos en G2\n",
    "\n",
    "df['G1'] = pd.to_numeric(df['G1'], errors='coerce')  # Convierte texto en G1 a float/int, errores -> NaN\n",
    "df['G2'] = pd.to_numeric(df['G2'], errors='coerce')  # Hace lo mismo en G2, garantizando uniformidad\n",
    "df['G3'] = pd.to_numeric(df['G3'], errors='coerce')  # También en G3, para asegurar tipos de datos consistentes\n",
    "\n",
    "# Convertir fecha\n",
    "df['fecha_evaluacion'] = pd.to_datetime(df['fecha_evaluacion'], format='%d/%m/%Y', errors='coerce')\n",
    "\n",
    "# 3.3 Eliminar columnas originales ya procesadas\n",
    "columnas_eliminar = ['student_info', 'studytime_absences']\n",
    "df = df.drop(columns=columnas_eliminar)\n",
    "\n",
    "print(f\"Nuevo dataset: {df.shape[0]} filas, {df.shape[1]} columnas\")\n",
    "\n",
    "# 4. DETECCIÓN Y TRATAMIENTO DE DATOS FALTANTES\n",
    "print(\"\\n4. ANÁLISIS DE DATOS FALTANTES\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# 4.1 Análisis completo de valores faltantes\n",
    "valores_faltantes = df.isnull().sum()\n",
    "porcentaje_faltantes = (valores_faltantes / len(df)) * 100\n",
    "\n",
    "print(\"Valores faltantes por columna:\")\n",
    "for col in df.columns:\n",
    "    if valores_faltantes[col] > 0:\n",
    "        print(f\"   {col}: {valores_faltantes[col]} ({porcentaje_faltantes[col]:.1f}%)\")\n",
    "\n",
    "# 4.2 Estrategias de imputación según tipo de variable\n",
    "print(\"\\nAplicando estrategias de imputación...\")\n",
    "\n",
    "# Recorremos todas las columnas del DataFrame\n",
    "for col in df.columns:\n",
    "    # Verificamos si la columna tiene valores nulos\n",
    "    if df[col].isnull().sum() > 0:\n",
    "        if df[col].dtype in ['int64', 'float64']:\n",
    "    \n",
    "            if abs(df[col].skew()) > 2:  # skew mide la asimetría de la distribución\n",
    "                impute_value = df[col].median()  # valor central menos afectado por outliers\n",
    "                metodo = \"mediana\"\n",
    "            else:\n",
    "                impute_value = df[col].mean()    # promedio aritmético\n",
    "                metodo = \"media\"\n",
    "\n",
    "            # Rellenamos los nulos con el valor calculado\n",
    "            df[col].fillna(impute_value, inplace=True)\n",
    "            print(f\"{col}: imputado con {metodo} ({impute_value:.2f})\")\n",
    "\n",
    "        #  2. Columnas Categóricas (tipo object)\n",
    "         \n",
    "        elif df[col].dtype == 'object':\n",
    "            impute_value = df[col].mode()[0] if not df[col].mode().empty else 'Desconocido'\n",
    "            df[col].fillna(impute_value, inplace=True)\n",
    "            print(f\"    {col}: imputado con moda ('{impute_value}')\")\n",
    "\n",
    "         \n",
    "        # 3. Columnas de Fechas (datetime)\n",
    "         \n",
    "        elif 'datetime' in str(df[col].dtype):\n",
    "            impute_value = df[col].mode()[0] if not df[col].mode().empty else df['fecha_evaluacion'].min()\n",
    "            df[col].fillna(impute_value, inplace=True)\n",
    "            print(f\"    {col}: imputado con fecha más común\")\n",
    "\n",
    "\n",
    "print(f\"Valores faltantes después de limpieza: {df.isnull().sum().sum()}\")\n",
    "\n",
    "# 5. DETECCIÓN Y ELIMINACIÓN DE DUPLICADOS\n",
    "print(\"\\n5. GESTIÓN DE DATOS DUPLICADO\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# 5.1 Buscar duplicados exactos\n",
    "duplicados_exactos = df.duplicated().sum()\n",
    "print(f\"Duplicados exactos encontrados: {duplicados_exactos}\")\n",
    "\n",
    "# 5.2 Buscar duplicados basados en clave natural (nombre + curso)\n",
    "if 'nombre_estudiante' in df.columns and 'curso' in df.columns:\n",
    "    duplicados_naturales = df.duplicated(subset=['nombre_estudiante', 'curso']).sum()\n",
    "    print(f\"Duplicados por estudiante-curso: {duplicados_naturales}\")\n",
    "\n",
    "# 5.3 Eliminar duplicados manteniendo el primer registro\n",
    "filas_antes = len(df)\n",
    "df = df.drop_duplicates()\n",
    "if 'nombre_estudiante' in df.columns and 'curso' in df.columns:\n",
    "    df = df.drop_duplicates(subset=['nombre_estudiante', 'curso'])\n",
    "filas_despues = len(df)\n",
    "\n",
    "print(f\"Filas eliminadas por duplicación: {filas_antes - filas_despues}\")\n",
    "print(f\"Dataset final: {filas_despues} filas\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
